---
title: "Expected Turnovers for Quarterbacks"
description: |
  Building expected interceptions and expected fumbles models to find QBs likely to increase or decrease their 
  interceptions and/or turnovers per dropback from 2019 to 2020.
author:
  - name: Anthony Gadaleta
    url: https://twitter.com/AG_8
date: 08-25-2020
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 3
repository_url: "https://github.com/mrcaseb/open-source-football"
categories:
  - Figures
  - nflfastR
  - turnovers
  - quarterbacks
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	dpi = 300,
	tidy = "styler"
)
```

## Intro

Outside of actually scoring points, few events can swing an NFL game the way an interception or fumble can. Over the course of a season, if your quarterback is continuously giving the football away to the other team, your odds of a successful season are likely quite low.

With that said, all turnovers, and specifically QB turnovers, are not created equal. The goal of expected interceptions, expected fumbles and (by adding them together) expected turnovers is to measure how likely an incomplete pass or fumble is to be converted to an interception or lost fumble.^[Without tracking data, it really doesn't make sense to calculate the likelihood of an interception or fumble on all plays.]

## Load Packages, Get the Data

```{r Load Packages}
library(nflfastR)
library(tidyverse)
library(caret)
library(ggimage)
library(gt)
```

Download the latest play-by-play data for all plays from 2006-2019. We'll be using 2006 as the start year because that's the first year we have air yards data fully accessible.

```{r Get pbp Data}
seasons = 2006:2019
pbp = purrr::map_df(seasons, function(x) {
  readRDS(
    url(
      glue::glue("https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds")
    )
  )
})
```

Download all NFL roster data from 1999-2019.

```{r Get Roster Data}
roster = readRDS(url("https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/roster-data/roster.rds"))
```

## Expected Interceptions

We'll start with building the model for expected interceptions. Our independent variables will be air yards, pass location, qb hits, number of pass defenders and season.

Start by creating an incompletions dataframe, which filters out all plays that do not result in incomplete passes or interceptions. Additionally, create the pass broken up (pbu) variable based on the number of pass defenders listed. The assumption being if more defenders are listed as defending a given pass, the more congested the throwing lane was.

```{r}
incompletions = pbp %>%
  filter(season_type == 'REG' & season >= 2006 & (incomplete_pass == 1 | interception == 1)) %>%
  select(incomplete_pass, air_yards, pass_defense_1_player_id, pass_defense_2_player_id,
         season, posteam, interception, qb_hit, week, defteam, passer, posteam, pass_location, desc) %>%
  mutate(
    pbu = case_when(
      !is.na(pass_defense_1_player_id) & !is.na(pass_defense_2_player_id) &
        (incomplete_pass == 1 | interception == 1) ~ 2,
      !is.na(pass_defense_1_player_id) & is.na(pass_defense_2_player_id) &
        (incomplete_pass == 1 | interception == 1) ~ 1,
      TRUE ~ 0
    ),
  )
incompletions$air_yards[is.na(incompletions$air_yards)] = 0
incompletions$pass_location[is.na(incompletions$pass_location)] = 'None'
```

Split into training and testing dataframes. I used 2006-2016 to train the model and 2017-2019 to test. The split comes out to approximately 79% training, 21% testing.

```{r}
int_train = incompletions %>%
  filter(season < 2017) %>%
  select(-c(pass_defense_1_player_id, pass_defense_2_player_id, incomplete_pass, posteam, week, defteam,
            passer, posteam, desc)) %>%
  mutate(
    interception = if_else(interception == 1, 'int', 'no.int'),
  )

int_test = incompletions %>%
  filter(season >= 2017)
```

Train the model using logistic regression, then add expected interceptions to the int_test dataframe.

```{r}
fitControl = trainControl(method = 'repeatedcv',
                          number = 10,
                          repeats = 10,
                          classProbs = TRUE,
                          summaryFunction = twoClassSummary)

set.seed(69) #nice
int_model = train(interception ~ ., data = int_train,
                  method = 'glm', preProcess = c('scale', 'center'),
                  metric = 'ROC', trControl = fitControl)
int_model

int_test$exp_int = predict(int_model, int_test, type = 'prob')
```

Let's take a look at the incompletions most likely have been intercepted from 2017-2019:

```{r}
int_test %>% 
  arrange(-exp_int) %>% 
  select(desc, exp_int, passer, posteam, defteam, season, week) %>% 
  head(5)
```

## Expected Fumbles

Next up, we'll build the expected fumbles model, using fumble forced vs not forced, fumble out of bounds, yards gained, sacks and aborted snaps as our independent variables.

Start by creating a fumbles dataframe, which includes all plays where the ball hits the turf, regardless of which team recovers.

```{r}
fumbles = pbp %>%
  filter(season_type == 'REG', season >=2006, fumble == 1) %>%
  select(fumble_forced, fumble_not_forced, fumble_out_of_bounds, fumble_lost, fumble, yards_gained, sack, 
         aborted_play, season, posteam, week, defteam, fumbled_1_player_name, desc)
```

Splitting the same way as the incompletions, this time for an 80-20 train-test split.

```{r}
fumble_train = fumbles %>%
  filter(season < 2017) %>%
  select(-c(season, posteam, week, fumble, defteam, fumbled_1_player_name, desc)) %>%
  mutate(
    fumble_lost = if_else(fumble_lost == 1, 'lost', 'recovered')
  )
fumble_test = fumbles %>%
  filter(season >= 2017)
```

Train the model using logistic regression (we can reuse the trControl from above) and then add expected fumbles to the fumble_test dataframe.

```{r}
set.seed(69) #nice
fumble_model = train(fumble_lost ~ ., data = fumble_train,
                     method = 'glm', preProcess = c('scale', 'center'),
                     trControl = fitControl, metric = 'ROC')
fumble_model

fumble_test$exp_fl = predict(fumble_model, fumble_test, type = 'prob')
```

Let's take a look at the fumbles most likely to have been lost from 2017-2019:

```{r}
fumble_test %>% 
  arrange(-exp_fl) %>% 
  select(desc, exp_fl, fumbled_1_player_name, posteam, defteam, season, week) %>% 
  head(5)
```

## Predictive Power?

Now it's time to see if these new stats are actually useful for predicting future turnovers.

First, modify roster names to allow us to use merge roster data with the dataframes we created above.

```{r}
roster$name = paste0(substr(roster$teamPlayers.firstName, 1, 1), '.', roster$teamPlayers.lastName)
```

Merge roster data and group by passer and season to get total interceptions and expected interceptions and then total fumbles lost and expected fumbles lost for each passer in each season.

```{r}
xInt = int_test %>%
  filter(!is.na(passer)) %>%
  left_join(roster[,c('team.season', 'name', 'teamPlayers.position', 'team.abbr',
                      'teamPlayers.headshot_url')],
            by=c('passer'='name', 'season'='team.season', 'posteam'='team.abbr')) %>%
  rename(
    position = teamPlayers.position,
    player = passer,
  ) %>%
  filter(position == 'QB') %>%
  group_by(player, posteam, season, teamPlayers.headshot_url) %>%
  summarise(Interceptions = sum(interception), xInt = sum(exp_int$int)) %>%
  mutate(diff = Interceptions - xInt)

xFmb = fumble_test %>%
  filter(!is.na(fumbled_1_player_name)) %>%
  left_join(roster[,c('team.season', 'name', 'teamPlayers.position', 'team.abbr',
                      'teamPlayers.headshot_url')],
            by=c('fumbled_1_player_name'='name', 'season'='team.season', 'posteam'='team.abbr')) %>%
  rename(
    position = teamPlayers.position,
    player = fumbled_1_player_name,
  ) %>%
  filter(position == 'QB') %>%
  group_by(player, posteam, season, teamPlayers.headshot_url) %>%
  summarise(Fumbles_Lost = sum(fumble_lost), xFmb = sum(exp_fl$lost)) %>%
  mutate(diff = Fumbles_Lost - xFmb)
```

Find total dropbacks, epa per dropback and success rate on dropbacks for each passer. The latter two stats really aren't necessary, but I thought it could be useful to show how well certain quarterbacks performed overall.

```{r}
dropbacks = pbp %>%
  filter(season_type == 'REG' & season > 2016 & !is.na(passer)) %>%
  group_by(passer, season) %>%
  summarise(dropbacks = n(), epa=mean(epa, na.rm=TRUE), sr=mean(success, na.rm=TRUE))
```

Merge the dropbacks dataframe with the xInt and xFmb dataframes. Then calc total turnovers, expected turnovers,  turnovers per dropback, interceptions per dropback, differences between all of the actual and expected stats and the next season's turnovers, interceptions and fumbles.

```{r}
xTO = dropbacks %>%
  inner_join(xInt, by=c('passer'='player', 'season')) %>%
  left_join(xFmb, by=c('passer'='player', 'posteam', 'season', 'teamPlayers.headshot_url'))

xTO$Fumbles_Lost[is.na(xTO$Fumbles_Lost)] = 0
xTO$xFmb[is.na(xTO$xFmb)] = 0
xTO$diff.y[is.na(xTO$diff.y)] = 0

xTO = xTO %>%
  mutate(
    Turnovers = Interceptions + Fumbles_Lost,
    xTO = xInt + xFmb,
    diff = diff.x + diff.y,
    to_pct = Turnovers / dropbacks,
    int_pct = Interceptions / dropbacks,
    xto_pct = xTO / dropbacks,
    xint_pct = xInt / dropbacks,
    to_pct_diff = xto_pct - to_pct,
    int_pct_diff = xint_pct - int_pct,
  ) %>%
  filter(dropbacks >= 250) %>%
  group_by(passer) %>%
  mutate(
    next_to = lead(Turnovers, 1),
    next_int = lead(Interceptions, 1),
    next_fmb = lead(Fumbles_Lost, 1),
    next_db = lead(dropbacks, 1)
  )
```

Finally, let's evaluate how predictive our new expected statistics are compared to their standard counterparts.

```{r echo=FALSE}
paste0("R2 of current TOs to next season's TOs: ", with(drop_na(xTO), cor(Turnovers, next_to)^2))
paste0("R2 of current xTO to next season's TOs: ", with(drop_na(xTO), cor(xTO, next_to)^2))
paste0("R2 of current Ints to next season's Ints: ", with(drop_na(xTO), cor(Interceptions, next_int)^2))
paste0("R2 of current xInts to next season's Ints: ", with(drop_na(xTO), cor(xInt, next_int)^2))
paste0("R2 of current Fumbles to next season's Fumbles: ", with(drop_na(xTO), cor(Fumbles_Lost, next_fmb)^2))
paste0("R2 of current xFmb to next season's Fumbles: ", with(drop_na(xTO), cor(xFmb, next_fmb)^2))
```

As we can see, the "regular" stats outperform all of the expected turnover statistics. But what if we look at these 
as rate stats per dropback?

```{r echo=FALSE}
paste0("R2 of current TOs per dropback to next season's TOs per dropback: ", 
       with(drop_na(xTO), cor(to_pct, next_to/next_db)^2))
paste0("R2 of current xTOs per dropback to next season's TOs per dropback: ",
       with(drop_na(xTO), cor(xto_pct, next_to/next_db)^2))
paste0("R2 of current Ints per dropback to next season's Ints per dropback: ",
       with(drop_na(xTO), cor(int_pct, next_int/next_db)^2))
paste0("R2 of current xInts per dropback to next season's Ints per dropback: ",
       with(drop_na(xTO), cor(xint_pct, next_int/next_db)^2))
```

Now we've got some winners! We can predict next season's turnovers and interceptions per dropback more effectively
using xTOs and xInts per dropback than we can using the ordinary rate stats.

## Visuals

Finally, we can plot the results to help us visualize who is most likely to turn the ball over at a higher or lower
rate next season.

```{r, layout="l-page", fig.height=5.5, preview=TRUE}
ggplot(subset(xTO, season==2019), aes(x=Turnovers/dropbacks, y=xTO/dropbacks)) +
  geom_image(aes(image = teamPlayers.headshot_url), size = 0.05, asp = 16 / 9) +
  labs(
    title = 'QB Turnovers 2019',
    subtitle = 'Regular Season | Min. 250 Dropbacks',
    x = 'Actual Turnovers per Dropback',
    y = 'Expected Turnovers per Dropback',
    caption = '@AG_8 | Data: @nflfastR'
  ) +
  theme_bw() +
  theme(
    aspect.ratio = 9 / 16,
    plot.title = element_text(size = 12, hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(size = 10, hjust = 0.5),
  ) +
  geom_abline(slope = 1, intercept = 0)
```

```{r, layout="l-page", fig.height=5.5}
ggplot(subset(xTO, season==2019), aes(x=Interceptions/dropbacks, y=xInt/dropbacks)) +
  geom_image(aes(image = teamPlayers.headshot_url), size = 0.05, asp = 16 / 9) +
  labs(
    title = 'QB Interceptions 2019',
    subtitle = 'Regular Season | Min. 250 Dropbacks',
    x = 'Actual Interceptions per Dropback',
    y = 'Expected Interceptions per Dropback',
    caption = '@AG_8 | Data: @nflfastR'
  ) +
  theme_bw() +
  theme(
    aspect.ratio = 9 / 16,
    plot.title = element_text(size = 12, hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(size = 10, hjust = 0.5),
  ) +
  geom_abline(slope = 1, intercept = 0)
```

Lastly, we'll use the gt package to make a really cool looking table of the 2019 data.

```{r, layout="l-page", fig.height=5.5}
xTO %>%
  ungroup() %>%
  filter(season == 2019) %>%
  select(c(passer, posteam, dropbacks, to_pct, xto_pct, to_pct_diff, int_pct, xint_pct, int_pct_diff)) %>%
  mutate(
    to_pct_diff = to_pct_diff * 100,
    int_pct_diff = int_pct_diff * 100
  ) %>%
  gt() %>%
  tab_header(
    title = "Expected QB Turnovers",
    subtitle = "Regular Season 2019 | Min. 250 Dropbacks"
  ) %>%
  cols_label(
    passer = "QB",
    posteam = "Team",
    dropbacks = "Dropbacks",
    to_pct = "TOs per Dropback",
    xto_pct = "xTOs per Dropback",
    to_pct_diff = "xTOs/DB - TOs/DB",
    int_pct = "Ints per Dropback",
    xint_pct = "xInts per Dropback",
    int_pct_diff = "xInts/DB - Ints/DB"
  ) %>%
  fmt_number(
    columns = c("to_pct", "xto_pct", "to_pct_diff", "int_pct", "xint_pct", "int_pct_diff"),
    decimals = 2
  ) %>%
  fmt_percent(
    columns = c("to_pct", "xto_pct", "int_pct", "xint_pct")
  ) %>%
  tab_source_note("@AG_8 | Data: @nflfastR") %>%
  data_color(
    columns = c("to_pct", "xto_pct", "to_pct_diff", "int_pct", "xint_pct", "int_pct_diff"),
    colors = scales::col_numeric(palette = 'Reds', domain = NULL)
  ) %>%
  cols_align(align = 'center') %>%
  cols_width(
    everything() ~ px(90)
  )
```

## Conclusion

Based on the table and plots above, we can see that Jameis had, by far, the largest difference between expected and actual turnovers. This really isn't much of a shock since you of course need some bad luck to have as high of a turnover rate as he did. 

Additionally, I found it interesting that although Rodgers and Wentz were two of "luckiest" with turnovers from last year, if they regress to their xTO and xInt numbers they would still be average or maybe even slightly above average, just in terms of turnovers.

Finally, although Daniel Jones and Kyle Allen were terrible in terms of hanging onto the rock last season, there isn't a ton of hope for improvement for either. Both had xTO rates just slightly below their actual TO rates.
